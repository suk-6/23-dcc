{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67859686-ca27-42d4-a52f-42020cf1399d",
   "metadata": {},
   "source": [
    "# 2023 데이터 크리에이터 캠프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8665a56-0611-4f0c-a941-0dca0a719052",
   "metadata": {},
   "source": [
    "## MISSION 1. AI-Hub 에서 데이터셋 다운로드 및 학습데이터와 테스트 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a75b2-470a-4855-aacf-f6db2bab8201",
   "metadata": {},
   "source": [
    "### 1-1. 첨부된 그림의 경로에서 TS7, TS8.ZIP는 Training VS2.ZIP는 Test 데이터셋으로 압축을 풀어 이미지 폴더를 만든다.\n",
    "### Training 과 Test 데이터의 이미지 수는 각각 얼마인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b633202-403d-47bc-b705-d1a5b4a66882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "trainZIP1 = 'TS7.zip'\n",
    "trainZIP2 = 'TS8.zip'\n",
    "testZIP = 'VS2.zip'\n",
    "trainPath = 'train'\n",
    "testPath = 'test'\n",
    "\n",
    "if not os.path.exists(trainPath):\n",
    "    os.makedirs(trainPath)\n",
    "\n",
    "if not os.path.exists(testPath):\n",
    "    os.makedirs(testPath)\n",
    "\n",
    "def extract_zip(zipPath, targetPath): # 압축을 해제하는 함수\n",
    "    with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(targetPath)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor: # 쓰레딩으로 동시 압축 해제\n",
    "    executor.submit(extract_zip, trainZIP1, trainPath)\n",
    "    executor.submit(extract_zip, trainZIP2, trainPath)\n",
    "    executor.submit(extract_zip, testZIP, testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469d9f84-1818-4b0c-80b2-adcc4356182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataPaths = ['train', 'test']\n",
    "datasetPath = os.getcwd()\n",
    "\n",
    "for dataPath in dataPaths:\n",
    "    folderPath = os.path.join(datasetPath, dataPath)\n",
    "    imageExtensions = ['.jpg', '.png', '.jpeg']\n",
    "    \n",
    "    imageCount = 0\n",
    "    \n",
    "    for _, _, files in os.walk(folderPath):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in imageExtensions):\n",
    "                imageCount += 1\n",
    "    \n",
    "    print(f\"{folderPath} 폴더에 {imageCount}개의 이미지가 있음.\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7ff9ba3-f358-477a-881f-aaa05c13730f",
   "metadata": {},
   "source": [
    "Output:\n",
    "D:\\2023dcc\\train 폴더에 129600개의 이미지가 있음.\n",
    "D:\\2023dcc\\test 폴더에 16200개의 이미지가 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2f8fd-043a-495b-85e2-d6a0f1b4f4b3",
   "metadata": {},
   "source": [
    "### 1-2. 이미지 이름에서 마지막 두자리가 01 : 마스크 미착용 02 : 마스크 부분 착용 03 : 마스크 착용을 의미한다.\n",
    "### Training, Test에 각각 몇 개가 있는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12321833-c522-45cf-a255-01e25766f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "suffixes = ['01', '02', '03']\n",
    "\n",
    "imageExtensions = ['.jpg', '.jpeg', '.png']\n",
    "\n",
    "dataPaths = ['train', 'test']\n",
    "datasetPath = os.getcwd()\n",
    "\n",
    "def countImage(dataPath):\n",
    "    path = os.path.join(datasetPath, dataPath)\n",
    "    imageCounts = {suffix: 0 for suffix in suffixes} # 라벨이 키인 딕셔너리를 정의\n",
    "    for _, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in imageExtensions):\n",
    "                try: # 파일 이름에서 라벨을 불러와 딕셔너리에 카운트\n",
    "                    index = file.lower().split('.')[0].split('_')[-1]\n",
    "                    imageCounts[index] += 1\n",
    "                except: # 실패 시 파일명 반환\n",
    "                    print(file)\n",
    "                    exit(1)\n",
    "\n",
    "    for suffix, count in imageCounts.items():\n",
    "        print(f\"{dataPath}에서 '{suffix}'로 끝나는 파일은 {count}개 있음.\")\n",
    "\n",
    "for dataPath in dataPaths:\n",
    "    countImage(dataPath)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a4159ae-bea4-4af5-8bba-a7a99f590bec",
   "metadata": {},
   "source": [
    "Output:\n",
    "train에서 '01'로 끝나는 파일은 43200개 있음.\n",
    "train에서 '02'로 끝나는 파일은 43200개 있음.\n",
    "train에서 '03'로 끝나는 파일은 43200개 있음.\n",
    "test에서 '01'로 끝나는 파일은 5400개 있음.\n",
    "test에서 '02'로 끝나는 파일은 5400개 있음.\n",
    "test에서 '03'로 끝나는 파일은 5400개 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010ee7d-ea75-4e0b-aa9a-95252ac46bdc",
   "metadata": {},
   "source": [
    "## MISSION 2. 이미지 데이터 축소하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca58cc9-e497-4808-ad30-5ac70fa96276",
   "metadata": {},
   "source": [
    "### 2-1. 사진 크기가 너무 크기 때문에 사진을 축소해야 Colab에서 학습할 수 있을 것이다. \n",
    "### 어떤 크기로 이미지를 축소해야 하는지 결정하고, 실제 축소된 이미지 폴더를 만드시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a59843-724e-4ef5-8031-7901883eb9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "trainImagePath = 'train'\n",
    "testImagePath = 'test'\n",
    "\n",
    "trainResizedPath = 'train_resize'\n",
    "testResizedPath = 'test_resize'\n",
    "\n",
    "imageExtensions = ['.jpg', '.png', '.jpeg']\n",
    "\n",
    "targetImageSize = (64, 64) # 이미지 사이즈\n",
    "\n",
    "if not os.path.exists(trainResizedPath):\n",
    "    os.makedirs(trainResizedPath)\n",
    "\n",
    "if not os.path.exists(testResizedPath):\n",
    "    os.makedirs(testResizedPath)\n",
    "\n",
    "def resize(originalPath, resizedPath): # 원본 폴더에 있는 사진을 지정된 폴더에 리사이징하여 저장하는 함수\n",
    "    for root, _, files in os.walk(originalPath):\n",
    "        for filename in files:\n",
    "            if any(filename.lower().endswith(ext) for ext in imageExtensions):\n",
    "                imagePath = os.path.join(root, filename)\n",
    "\n",
    "                relativePath = os.path.relpath(imagePath, originalPath)\n",
    "                outputPath = os.path.join(resizedPath, relativePath) # 디렉토리 계산해서 저장 경로 지정\n",
    "\n",
    "                os.makedirs(os.path.dirname(outputPath), exist_ok=True)\n",
    "                \n",
    "                img = cv2.imread(imagePath) # OpenCV로 이미지 읽어오기\n",
    "                img = cv2.resize(img, targetImageSize, interpolation=cv2.INTER_AREA) # OpenCV로 이미지 리사이징\n",
    "                cv2.imwrite(outputPath, img) # 이미지 저장\n",
    "\n",
    "resize(trainImagePath, trainResizedPath)\n",
    "resize(testImagePath, testResizedPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d000ae0-b6fe-4731-9782-62b5b79f7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "trainResizedPath = 'train_resize'\n",
    "testResizedPath = 'test_resize'\n",
    "\n",
    "imageExtensions = ['.jpg', '.png', '.jpeg']\n",
    "\n",
    "\n",
    "def datasetToNumpy(datasetPath, targetSize): # 이미지를 읽어와서 넘파이 배열로 변환하는 함수\n",
    "    imageData = []\n",
    "    labelData = []\n",
    "    \n",
    "    for root, _, files in os.walk(datasetPath):\n",
    "        for filename in tqdm(files):\n",
    "            if any(filename.lower().endswith(ext) for ext in imageExtensions):\n",
    "                imgPath = os.path.join(root, filename)\n",
    "                img = Image.open(imgPath)\n",
    "                img = img.resize(targetSize)\n",
    "                imgArray = np.array(img)\n",
    "                \n",
    "                index = filename.split('.')[0].split('_')[-1]\n",
    "                \n",
    "                imageData.append(imgArray)\n",
    "                labelData.append(int(index))\n",
    "    \n",
    "    return np.array(imageData), np.array(labelData) # 이미지 데이터와 라벨 데이터를 각각의 넘파이 배열로 저장\n",
    "\n",
    "# targetSize = (256, 256)\n",
    "targetSize = (64, 64)\n",
    "\n",
    "trainImages, trainLabels = datasetToNumpy(trainResizedPath, targetSize)\n",
    "testImages, testLabels = datasetToNumpy(testResizedPath, targetSize)\n",
    "\n",
    "# 변환된 넘파이 배열을 각각의 npy 파일로 저장\n",
    "np.save('train_images.npy', trainImages)\n",
    "np.save('train_labels.npy', trainLabels)\n",
    "np.save('test_images.npy', testImages)\n",
    "np.save('test_labels.npy', testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f17a7-ad8d-4c92-b1b6-4f4703a24282",
   "metadata": {},
   "source": [
    "### 2-2. 이미지 파일을 Colab으로 옮기는 것은 여러가지로 어렵다. 두가지 방식을 사용할 수 있다. 적절한 방법을 택해서 수행하시오.\n",
    "\n",
    "(1) 로컬 컴퓨터에서 이미지를 넘파이 배열로 만들어 Colab으로 전송하는 방법 \n",
    "\n",
    "(2) 이미지를 하나의 압축파일로 만들어 Colab으로 전송하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b22fab-eb12-48dd-92b8-948b751d17a5",
   "metadata": {},
   "source": [
    "2-1에서 1번 방법을 사용하여, numpy 배열로 변환 후 파일로 저장."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd355e4-36f2-46a7-b66a-5713d1e86b1e",
   "metadata": {},
   "source": [
    "## MISSION 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f5e06-9b69-478a-9463-62420fb5f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset, ConcatDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980733a-b28a-4e68-a70e-1841f64914af",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"23-dcc\", name = 'CenterCrop (50 -> 32), Conv3, Do1(0.15)')\n",
    "\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed72507-6914-4b45-854a-e45bb03efb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 넘파이 배열 불러오기\n",
    "train_images = np.load('train_images.npy')\n",
    "train_labels = np.load('train_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56738fe0-fdae-4203-b7e0-86dfac297c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 수정: 1번 레이블을 0(마스크 미착용), 2/3번 레이블을 1(마스크 착용)로 변경\n",
    "train_labels[train_labels == 1] = 0\n",
    "train_labels[train_labels == 2] = 1\n",
    "train_labels[train_labels == 3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5827a-6587-40f8-9a42-26b788afff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11458596-8f0f-4463-af7c-d16bf52c9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강을 위한 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.CenterCrop((50,50)),\n",
    "    transforms.Resize((32,32))\n",
    "])\n",
    "\n",
    "# 데이터셋 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None, select_label=None, limit=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.select_label = select_label\n",
    "        self.limit = limit\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.limit is not None:\n",
    "            return self.limit\n",
    "            \n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.select_label is not None and label != self.select_label:\n",
    "            return self.__getitem__(idx + 1)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(TF.to_pil_image(image))\n",
    "            image = TF.to_tensor(image)\n",
    "        else:\n",
    "            image = TF.to_tensor(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# original_dataset = CustomDataset(train_images, train_labels)\n",
    "augmented_dataset = CustomDataset(train_images, train_labels, transform)\n",
    "nomask_added_dataset = CustomDataset(train_images, train_labels, transform, select_label=0, limit=(sum(1 for label in train_labels if label == 1) - sum(1 for label in train_labels if label == 0)))\n",
    "\n",
    "dataset = ConcatDataset([augmented_dataset, nomask_added_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19bd73-dcdb-4198-8f75-61abcc2dcdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 train과 validation 세트로 나누기\n",
    "train_size = int(0.7 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# 데이터로더 설정\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d2ae2-3bbc-4688-ae51-1af763ed2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_class_count = [0, 0]\n",
    "combined_class_count = [0, 0]\n",
    "\n",
    "for label in tqdm(train_labels):\n",
    "    if label == 0:\n",
    "        original_class_count[0] += 1\n",
    "    if label == 1:\n",
    "        original_class_count[1] += 1\n",
    "\n",
    "for _, label in tqdm(dataset):\n",
    "    if label == 0:\n",
    "        combined_class_count[0] += 1\n",
    "    if label == 1:\n",
    "        combined_class_count[1] += 1\n",
    "\n",
    "# 시각화\n",
    "labels = ['no mask', 'mask']\n",
    "original_counts = [original_class_count[0], original_class_count[1]]\n",
    "combined_counts = [combined_class_count[0], combined_class_count[1]]\n",
    "\n",
    "bar_width = 0.35\n",
    "index = range(len(labels))\n",
    "\n",
    "plt.bar(index, original_counts, bar_width, label='Original Dataset', color='blue')\n",
    "plt.bar([i + bar_width for i in index], combined_counts, bar_width, label='Combined Dataset', color='orange')\n",
    "\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Comparison of Dataset Lengths by Class')\n",
    "plt.xticks([i + bar_width / 2 for i in index], labels)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig('comparison-of-dataset.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5929a0f-513f-4241-bb83-9adce9ed23dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 분류 모델 정의\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(0.15)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 128)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = CustomClassifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "201cdb82-fe71-44b2-9990-105c5548b18a",
   "metadata": {},
   "source": [
    "Output:\n",
    "CustomClassifier(\n",
    "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
    "  (relu1): ReLU()\n",
    "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (dropout1): Dropout(p=0.15, inplace=False)\n",
    "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
    "  (relu2): ReLU()\n",
    "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
    "  (relu3): ReLU()\n",
    "  (fc1): Linear(in_features=2048, out_features=128, bias=True)\n",
    "  (relu4): ReLU()\n",
    "  (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
    "  (relu5): ReLU()\n",
    "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
    "  (sigmoid): Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955cef5-4b68-4506-a1f9-ab9d54085943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss().to(device)  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "wandb.watch(model, criterion, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1296eacb-f917-482b-b7b6-75d232961dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and validation functions\n",
    "def train(model, train_loader, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"epoch {epoch}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view_as(outputs).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273fd119-19f9-4044-b453-fd8918b55160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, valid_loader):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(valid_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.view_as(outputs).float())\n",
    "            valid_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).squeeze()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.byte()).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return valid_loss / len(valid_loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce20181-f0c7-4c6f-babb-93e086f3fac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "wandb.config.update({\n",
    "  \"learning_rate\": learning_rate,\n",
    "  \"epochs\": epochs,\n",
    "  \"batch_size\": batch_size\n",
    "})\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    wandb.config.update({\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "    \n",
    "    train_loss = train(model, train_loader, epoch)\n",
    "    valid_loss, valid_accuracy = validate(model, valid_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    wandb.log({\"train\": {\"loss\": train_loss}, \"val\": {\"loss\": valid_loss, \"acc\": valid_accuracy}}, step=epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "    print(f\"  Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {valid_loss:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {valid_accuracy:.2f}%\")\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "if os.path.exists('./weights') == False:\n",
    "    os.mkdir('./weights')\n",
    "\n",
    "torch.save(model.state_dict(), f'./weights/{now.strftime(\"%Y-%m-%d_%H:%M:%S\")}.pth')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d542d-e39d-47c5-b0d1-f63077caead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs + 1), valid_losses, label='Valid Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e42b1a-ecce-459a-9980-56e98d663a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 테스트 넘파이 배열 불러오기\n",
    "test_images = np.load('test_images.npy')\n",
    "test_labels = np.load('test_labels.npy')\n",
    "\n",
    "# 레이블 수정: 1번 레이블을 0(마스크 미착용), 2/3번 레이블을 1(마스크 착용)로 변경\n",
    "test_labels[test_labels == 1] = 0\n",
    "test_labels[test_labels == 2] = 1\n",
    "test_labels[test_labels == 3] = 1\n",
    "\n",
    "test_images = torch.from_numpy(test_images).permute(0, 3, 1, 2)\n",
    "test_labels = torch.from_numpy(test_labels)\n",
    "\n",
    "# Create a custom dataset\n",
    "test_dataset = TensorDataset(test_images, test_labels)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 1\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f840b163-d69c-41b8-923b-6851563f4a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 이터레이터로 변환\n",
    "data_iterator = iter(test_loader)\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "classes = ['no mask', 'mask']\n",
    "\n",
    "# Display the first 10 images and labels using iter and next\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    try:\n",
    "        image, label = next(data_iterator)\n",
    "        image = transforms.Resize((32, 32))(image)\n",
    "\n",
    "        # Forward pass through the model to get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image.to(torch.float))\n",
    "\n",
    "        # Convert the output to a probability by applying sigmoid (assuming binary classification)\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "\n",
    "        # Get the predicted class (0 or 1)\n",
    "        print(probabilities)\n",
    "        predicted_class = torch.round(probabilities).squeeze().int().item()\n",
    "        \n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(image[0].permute(1, 2, 0) / 255.0)\n",
    "        plt.title(f'Predicted: {classes[predicted_class]}\\nActual: {classes[int(label.item())]}')\n",
    "        plt.axis('off')\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27ac2b-09c1-43f3-a8a5-92e2e1a86e52",
   "metadata": {},
   "source": [
    "### 3-1. 마스크 착용여부 이진 분류를 수행하여 Test 데이터의 F1 Score를 제시하시오.\n",
    "\n",
    "(이 때 턱스크는 착용으로 간주)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b9cfb-5331-4912-951f-aacfc246846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "model = CustomClassifier()\n",
    "model.load_state_dict(torch.load(f'./weights/{now.strftime(\"%Y-%m-%d_%H:%M:%S\")}.pth'))\n",
    "\n",
    "def get_predictions(model, data_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = transforms.Resize((32, 32))(inputs)\n",
    "            outputs = model(inputs.to(torch.float))\n",
    "            predicted_labels = (torch.sigmoid(outputs) > 0.5).int()\n",
    "            all_predictions.extend(predicted_labels.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_predictions, all_labels\n",
    "\n",
    "predictions, labels = get_predictions(model, test_loader)\n",
    "report = classification_report(labels, predictions, target_names=classes)\n",
    "print(report)\n",
    "print(f\"F1 Score: {f1_score(labels, predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf6063f",
   "metadata": {},
   "source": [
    "Output:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "     no mask       0.94      0.90      0.92      5400\n",
    "        mask       0.95      0.97      0.96     10800\n",
    "\n",
    "    accuracy                           0.95     16200\n",
    "   macro avg       0.94      0.93      0.94     16200\n",
    "weighted avg       0.95      0.95      0.95     16200\n",
    "\n",
    "F1 Score: 0.9598"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d6601-3a5d-442c-b369-4487e587121e",
   "metadata": {},
   "source": [
    "### 3-2. 클래스 불균형 문제를 해결하고 Accuracy와 Precision, Recall, F1 Score를 계산해본 뒤 3-1.과 비교하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b2b7e-1bc5-4261-bb26-de855f055c91",
   "metadata": {},
   "source": [
    "클래스 불균형 해결 전:\n",
    "Accuracy: 0.9183\n",
    "Precision: 0.9787\n",
    "Recall: 0.8970\n",
    "F1 Score: 0.9361\n",
    "\n",
    "클래스 불균형 해결 후:\n",
    "Accuracy: 0.9421\n",
    "Precision: 0.9810\n",
    "Recall: 0.9312\n",
    "F1 Score: 0.9554"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a884bf76-dca4-4b3b-b8f3-45b8bb17aabb",
   "metadata": {},
   "source": [
    "### 3-3. 모형의 인식 성능을 올리는 작업을 수행하고 작업 수행과정을 설명하시오. 어떤 수행을 했고, 정확도는 어떻게 변했는가? 왜 그런 결과가 나왔는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835510b-9119-4ad1-8696-ada24a3f6d06",
   "metadata": {},
   "source": [
    "1. 하이퍼파라미터 조정\n",
    "    - 이동 평균을 사용하여 학습률을 동적으로 조정할 수 있도록 Adam 학습 알고리즘(옵티마이저)를 사용했습니다.\n",
    "    - 가중치가 업데이트되는 주기인 Learning Rate를 조절하여 파라미터가 일반화되게 할 수 있도록 했습니다.\n",
    "    - 연산 부하를 줄이기 위해 이미지 배치 사이즈를 조절하여 Loss Function을 효율적으로 최적화했습니다.\n",
    "\n",
    "<br />\n",
    "\n",
    "2. 모델 아키텍처 개선\n",
    "    - 레이어의 구조와 크기, 컨볼루션 필터 수, 히든 레이어 유닛 수 등을 조정하여 정교한 모델을 제작했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375c26b",
   "metadata": {},
   "source": [
    "### 3-4. 오류가 나온 이미지에 대해 왜 오류가 나왔는지 그동안 미션 수행에서 얻은 경험과 지식을 통해 설명하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625466f-e893-4050-9e57-35c240ef18c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
